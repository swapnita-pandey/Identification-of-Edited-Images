{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVg4zR-GIRkL"
      },
      "source": [
        "# Fake Image Detection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fodd-0kSIRkQ"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qn4Bm2xBIRkQ",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#to generate same sequence of random numbers in whole file\n",
        "np.random.seed(2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#It is used during the training of a neural network to monitor a \n",
        "# specified metric (typically a validation metric) and stop training\n",
        "#  early if certain criteria are met. The purpose of using EarlyStopping\n",
        "#  is to prevent overfitting and to save time and resources when further\n",
        "#  training is unlikely to improve the model's performance.\n",
        "from keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PIL provides extensive capabilities for opening, manipulating, and saving many different image file formats.\n",
        "\n",
        "to open image\n",
        "\n",
        "ImageEnhance =>  can manipulate pixel colors in images, apply color mapping, and adjust brightness, contrast, and other color-related properties.\n",
        "\n",
        "ImageChops => It provides various arithmetic and logical operations that you can perform on images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_IJUVsvqIRkT",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from pylab import *\n",
        "import re\n",
        "from PIL import Image, ImageChops, ImageEnhance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk0SXUo_IRkU"
      },
      "source": [
        "## Making Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXjFUHxWIRkW"
      },
      "source": [
        "## Convert to Error Level Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, this code takes an input image, compresses it, calculates the difference between the original and compressed versions, scales the difference image, and enhances its brightness. The resulting ELA image highlights areas where digital manipulation may have occurred by making the manipulated regions appear as brighter or darker areas compared to the rest of the image.\n",
        "\n",
        "It generates ela_Image by enhancing brightness on the scale of pixel difference(difference between original and converted RGB image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZdBxF3y6IRkX",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def convert_to_ela_image(path, quality):\n",
        "    # creating a temporary filename for an intermediate image\n",
        "    temp_filename = 'temp_file_name.jpg'\n",
        "    # filename for ela image that will be generated\n",
        "    ela_filename = 'temp_ela.png'\n",
        "    \n",
        "    # open image and convert to RGB\n",
        "    image = Image.open(path).convert('RGB')\n",
        "\n",
        "    # save image as jpg and keep quality as before\n",
        "    image.save(temp_filename, 'JPEG', quality = quality)\n",
        "    temp_image = Image.open(temp_filename)\n",
        "    \n",
        "    # calculate pixel difference between original image and RGB (new image) \n",
        "    # which will represents areas of image that have been altered.\n",
        "    ela_image = ImageChops.difference(image, temp_image)\n",
        "    \n",
        "    # calculating minimum and maximum pixel values in the images\n",
        "    extrema = ela_image.getextrema()\n",
        "\n",
        "    # finds the maximum difference value among the extrema. This value is used to scale the ELA image.\n",
        "    max_diff = max([ex[1] for ex in extrema])\n",
        "\n",
        "    # ensuring max_diff is not zero to avoid division by zero.\n",
        "    if max_diff == 0:\n",
        "        max_diff = 1\n",
        "\n",
        "    # calculates a scaling factor based on the maximum difference value. This factor\n",
        "    # is used to stretch the ELA image's pixel values across the full 0-255 range.\n",
        "    scale = 255.0 / max_diff\n",
        "    \n",
        "    # enhances the brightness of the ELA image by applying the previously calculated \n",
        "    # scaling factor for making the manipulated regions stand out more distinctly.\n",
        "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
        "    \n",
        "    return ela_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyRtK_DIRka"
      },
      "source": [
        "\n",
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbV4G3ifIRkb"
      },
      "source": [
        "### Read dataset and conversion to ELA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LLd2r5toIRkb",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "image_size = (128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function converts the image into ela_image then resize it and flattens it to store in the 1D array as CNN requires data to be in 1D array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SHLbguFLMlrO"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image_path):\n",
        "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UuLhY_4tIRkc",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function processes all the files present in Au and convert them:\n",
        "\n",
        "first into ela_image and after resizing it, it flattens the layers of image and converts in 1D array\n",
        "\n",
        "X stores all the flattened images and Y store the count of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j49G13jOIRkc",
        "outputId": "9d0a0f6c-204d-4507-c838-8e721a94fea6",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 500 images\n",
            "Processing 1000 images\n",
            "Processing 1500 images\n",
            "Processing 2000 images\n",
            "Processing 2500 images\n",
            "Processing 3000 images\n",
            "Processing 3500 images\n",
            "Processing 4000 images\n",
            "Processing 4500 images\n",
            "Processing 5000 images\n",
            "5123 5123\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "path= \"D:/major_project/dataset/real\"\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        # if filename.endswith('jpg') or filename.endswith('png') or filename.endswith('tif'):\n",
        "            full_path = os.path.join(dirname, filename)\n",
        "            X.append(prepare_image(full_path))\n",
        "            Y.append(1)\n",
        "            if len(Y) % 500 == 0:\n",
        "                print(f'Processing {len(Y)} images')\n",
        "\n",
        "random.shuffle(X)\n",
        "# X = X[:2100]\n",
        "# Y = Y[:2100]\n",
        "print(len(X), len(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function processes all the files present in Tp and convert them:\n",
        "\n",
        "first into ela_image and after resizing it, it flattens the layers of image and converts in 1D array\n",
        "\n",
        "X stores all the flattened images and Y store the count of images\n",
        "\n",
        "now X stores all the images(Au + Tp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGitNgdJIRkd",
        "outputId": "4f023d73-e678-4be2-87f1-b4c89dbc0d88",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 5500 images\n",
            "Processing 6000 images\n",
            "Processing 6500 images\n",
            "Processing 7000 images\n",
            "Processing 7500 images\n",
            "Processing 8000 images\n",
            "Processing 8500 images\n",
            "Processing 9000 images\n",
            "Processing 9500 images\n",
            "Processing 10000 images\n",
            "10246 10246\n"
          ]
        }
      ],
      "source": [
        "path = \"D:/major_project/dataset/fake\"\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Image_Detector/CASIA2/Fake'\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        # if filename.endswith('jpg') or filename.endswith('png'):\n",
        "            full_path = os.path.join(dirname, filename)\n",
        "            X.append(prepare_image(full_path))\n",
        "            Y.append(0)\n",
        "            if len(Y) % 500 == 0:\n",
        "                print(f'Processing {len(Y)} images')\n",
        "\n",
        "print(len(X), len(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7mBhluLlIRkd",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# converting X into array \n",
        "X = np.array(X)\n",
        "#used when you have a target variable (labels or classes) that is represented as integers and you want to convert it into a binary matrix format suitable for training machine learning models, especially neural networks.\n",
        "# In Y there are two classes 1 for real and 0 for fake so to_categorical() converts this array into 2D array of labelled classes.\n",
        "# this Y will look like [[1,0],[0,1]] for 0 and 1 label.\n",
        "Y = to_categorical(Y, 2)\n",
        "# # Reshape image data for a convolutional neural network\n",
        "#  # Batch size, height, width, channels\n",
        "X = X.reshape(-1, 128, 128, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the VGG16 model with pre-trained weights (excluding the top classification layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sequential model and add the VGG16 base model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "\n",
        "# Flatten the output from the base model\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with softmax activation (2 classes: fake and real)\n",
        "model.add(Dense(2, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split into 9 : 1 then 8 : 2 then 7 : 3\n",
        "# test_size = 0.1 then 0.2 then 0.3\n",
        "# Save model when test_size = 0.2 only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the dataset into features (X) and target labels (Y)\n",
        "\n",
        "Splitting the data into a training set (80%) and a testing set (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8196 8196\n",
            "2050 2050\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
        "# again converting 2D array to 1D array\n",
        "X = X.reshape(-1,1,1,1)\n",
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining early stopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
        "                              min_delta = 0,\n",
        "                              patience = 2,\n",
        "                              verbose = 0,\n",
        "                              mode = 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 2474s 10s/step - loss: 0.4268 - accuracy: 0.8169 - val_loss: 0.4293 - val_accuracy: 0.8122\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating the accuracy on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\swapn\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('vgg16.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 158s 2s/step\n",
            "Overall Accuracy on Validation Set: 81.219512%\n",
            "Classification Report on Validation Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.85      0.76      0.80      1009\n",
            "        real       0.79      0.87      0.82      1041\n",
            "\n",
            "    accuracy                           0.81      2050\n",
            "   macro avg       0.82      0.81      0.81      2050\n",
            "weighted avg       0.82      0.81      0.81      2050\n",
            "\n",
            "Confusion Matrix on Validation Set:\n",
            "[[763 246]\n",
            " [139 902]]\n",
            "False Positive Rate (FPR): 24.380575%\n",
            "False Negative Rate (FNR): 13.352546%\n",
            "Cohen's Kappa on Validation Set: 0.623685\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
        "class_names = ['fake', 'real']\n",
        "# After training the model, you can use X_val and Y_val for evaluation\n",
        "Y_pred_val = model.predict(X_val)\n",
        "\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes_val = np.argmax(Y_pred_val, axis=1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true_val = np.argmax(Y_val, axis=1)\n",
        "\n",
        "# Calculate overall accuracy on validation set\n",
        "overall_accuracy_val = accuracy_score(Y_true_val, Y_pred_classes_val)\n",
        "print(f'Overall Accuracy on Validation Set: {overall_accuracy_val * 100:.6f}%')\n",
        "\n",
        "# Calculate other metrics using classification_report\n",
        "report_val = classification_report(Y_true_val, Y_pred_classes_val, target_names=class_names)\n",
        "print('Classification Report on Validation Set:')\n",
        "print(report_val)\n",
        "\n",
        "# Calculate confusion matrix on validation set\n",
        "conf_matrix_val = confusion_matrix(Y_true_val, Y_pred_classes_val)\n",
        "print('Confusion Matrix on Validation Set:')\n",
        "print(conf_matrix_val)\n",
        "\n",
        "true_positive = conf_matrix_val[1, 1]\n",
        "false_positive = conf_matrix_val[0, 1]\n",
        "false_negative = conf_matrix_val[1, 0]\n",
        "true_negative = conf_matrix_val[0, 0]\n",
        "\n",
        "# Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "fpr = false_positive / (false_positive + true_negative)\n",
        "fnr = false_negative / (false_negative + true_positive)\n",
        "\n",
        "print(f'False Positive Rate (FPR): {fpr * 100:.6f}%')\n",
        "print(f'False Negative Rate (FNR): {fnr * 100:.6f}%')\n",
        "\n",
        "# Calculate Cohen's Kappa on validation set\n",
        "cohen_kappa_val = cohen_kappa_score(Y_true_val, Y_pred_classes_val)\n",
        "print(f'Cohen\\'s Kappa on Validation Set: {cohen_kappa_val:.6f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Fake Image .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
