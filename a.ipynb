{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVg4zR-GIRkL"
      },
      "source": [
        "# Fake Image Detection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fodd-0kSIRkQ"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qn4Bm2xBIRkQ",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#to generate same sequence of random numbers in whole file\n",
        "np.random.seed(2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#It is used during the training of a neural network to monitor a \n",
        "# specified metric (typically a validation metric) and stop training\n",
        "#  early if certain criteria are met. The purpose of using EarlyStopping\n",
        "#  is to prevent overfitting and to save time and resources when further\n",
        "#  training is unlikely to improve the model's performance.\n",
        "from keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PIL provides extensive capabilities for opening, manipulating, and saving many different image file formats.\n",
        "\n",
        "to open image\n",
        "\n",
        "ImageEnhance =>  can manipulate pixel colors in images, apply color mapping, and adjust brightness, contrast, and other color-related properties.\n",
        "\n",
        "ImageChops => It provides various arithmetic and logical operations that you can perform on images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_IJUVsvqIRkT",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from pylab import *\n",
        "import re\n",
        "from PIL import Image, ImageChops, ImageEnhance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk0SXUo_IRkU"
      },
      "source": [
        "## Making Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXjFUHxWIRkW"
      },
      "source": [
        "## Convert to Error Level Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, this code takes an input image, compresses it, calculates the difference between the original and compressed versions, scales the difference image, and enhances its brightness. The resulting ELA image highlights areas where digital manipulation may have occurred by making the manipulated regions appear as brighter or darker areas compared to the rest of the image.\n",
        "\n",
        "It generates ela_Image by enhancing brightness on the scale of pixel difference(difference between original and converted RGB image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZdBxF3y6IRkX",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def convert_to_ela_image(path, quality):\n",
        "    # creating a temporary filename for an intermediate image\n",
        "    temp_filename = 'temp_file_name.jpg'\n",
        "    # filename for ela image that will be generated\n",
        "    ela_filename = 'temp_ela.png'\n",
        "    \n",
        "    # open image and convert to RGB\n",
        "    image = Image.open(path).convert('RGB')\n",
        "\n",
        "    # save image as jpg and keep quality as before\n",
        "    image.save(temp_filename, 'JPEG', quality = quality)\n",
        "    temp_image = Image.open(temp_filename)\n",
        "    \n",
        "    # calculate pixel difference between original image and RGB (new image) \n",
        "    # which will represents areas of image that have been altered.\n",
        "    ela_image = ImageChops.difference(image, temp_image)\n",
        "    \n",
        "    # calculating minimum and maximum pixel values in the images\n",
        "    extrema = ela_image.getextrema()\n",
        "\n",
        "    # finds the maximum difference value among the extrema. This value is used to scale the ELA image.\n",
        "    max_diff = max([ex[1] for ex in extrema])\n",
        "\n",
        "    # ensuring max_diff is not zero to avoid division by zero.\n",
        "    if max_diff == 0:\n",
        "        max_diff = 1\n",
        "\n",
        "    # calculates a scaling factor based on the maximum difference value. This factor\n",
        "    # is used to stretch the ELA image's pixel values across the full 0-255 range.\n",
        "    scale = 255.0 / max_diff\n",
        "    \n",
        "    # enhances the brightness of the ELA image by applying the previously calculated \n",
        "    # scaling factor for making the manipulated regions stand out more distinctly.\n",
        "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
        "    \n",
        "    return ela_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyRtK_DIRka"
      },
      "source": [
        "\n",
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbV4G3ifIRkb"
      },
      "source": [
        "### Read dataset and conversion to ELA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LLd2r5toIRkb",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "image_size = (128, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function converts the image into ela_image then resize it and flattens it to store in the 1D array as CNN requires data to be in 1D array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SHLbguFLMlrO"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image_path):\n",
        "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UuLhY_4tIRkc",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function processes all the files present in Au and convert them:\n",
        "\n",
        "first into ela_image and after resizing it, it flattens the layers of image and converts in 1D array\n",
        "\n",
        "X stores all the flattened images and Y store the count of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j49G13jOIRkc",
        "outputId": "9d0a0f6c-204d-4507-c838-8e721a94fea6",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 500 images\n",
            "Processing 1000 images\n",
            "Processing 1500 images\n",
            "Processing 2000 images\n",
            "Processing 2500 images\n",
            "Processing 3000 images\n",
            "Processing 3500 images\n",
            "Processing 4000 images\n",
            "Processing 4500 images\n",
            "Processing 5000 images\n",
            "5123 5123\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "path= \"D:/major_project/dataset/real\"\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        # if filename.endswith('jpg') or filename.endswith('png') or filename.endswith('tif'):\n",
        "            full_path = os.path.join(dirname, filename)\n",
        "            X.append(prepare_image(full_path))\n",
        "            Y.append(1)\n",
        "            if len(Y) % 500 == 0:\n",
        "                print(f'Processing {len(Y)} images')\n",
        "\n",
        "random.shuffle(X)\n",
        "# X = X[:2100]\n",
        "# Y = Y[:2100]\n",
        "print(len(X), len(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function processes all the files present in Tp and convert them:\n",
        "\n",
        "first into ela_image and after resizing it, it flattens the layers of image and converts in 1D array\n",
        "\n",
        "X stores all the flattened images and Y store the count of images\n",
        "\n",
        "now X stores all the images(Au + Tp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGitNgdJIRkd",
        "outputId": "4f023d73-e678-4be2-87f1-b4c89dbc0d88",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 5500 images\n",
            "Processing 6000 images\n",
            "Processing 6500 images\n",
            "Processing 7000 images\n",
            "Processing 7500 images\n",
            "Processing 8000 images\n",
            "Processing 8500 images\n",
            "Processing 9000 images\n",
            "Processing 9500 images\n",
            "Processing 10000 images\n",
            "10246 10246\n"
          ]
        }
      ],
      "source": [
        "path = \"D:/major_project/dataset/fake\"\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Image_Detector/CASIA2/Fake'\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        # if filename.endswith('jpg') or filename.endswith('png'):\n",
        "            full_path = os.path.join(dirname, filename)\n",
        "            X.append(prepare_image(full_path))\n",
        "            Y.append(0)\n",
        "            if len(Y) % 500 == 0:\n",
        "                print(f'Processing {len(Y)} images')\n",
        "\n",
        "print(len(X), len(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7mBhluLlIRkd",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# converting X into array \n",
        "X = np.array(X)\n",
        "#used when you have a target variable (labels or classes) that is represented as integers and you want to convert it into a binary matrix format suitable for training machine learning models, especially neural networks.\n",
        "# In Y there are two classes 1 for real and 0 for fake so to_categorical() converts this array into 2D array of labelled classes.\n",
        "# this Y will look like [[1,0],[0,1]] for 0 and 1 label.\n",
        "Y = to_categorical(Y, 2)\n",
        "# # Reshape image data for a convolutional neural network\n",
        "#  # Batch size, height, width, channels\n",
        "X = X.reshape(-1, 128, 128, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the VGG16 model with pre-trained weights (excluding the top classification layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sequential model and add the VGG16 base model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "\n",
        "# Flatten the output from the base model\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with softmax activation (2 classes: fake and real)\n",
        "model.add(Dense(2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split into 9 : 1 then 8 : 2 then 7 : 3\n",
        "# test_size = 0.1 then 0.2 then 0.3\n",
        "# Save model when test_size = 0.2 only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the dataset into features (X) and target labels (Y)\n",
        "\n",
        "Splitting the data into a training set (80%) and a testing set (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8196 8196\n",
            "2050 2050\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
        "# again converting 2D array to 1D array\n",
        "X = X.reshape(-1,1,1,1)\n",
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining early stopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
        "                              min_delta = 0,\n",
        "                              patience = 2,\n",
        "                              verbose = 0,\n",
        "                              mode = 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 2488s 10s/step - loss: 0.4417 - accuracy: 0.8125 - val_loss: 0.3832 - val_accuracy: 0.8405\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 2583s 10s/step - loss: 0.3704 - accuracy: 0.8592 - val_loss: 0.3383 - val_accuracy: 0.8683\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 1905s 7s/step - loss: 0.3357 - accuracy: 0.8784 - val_loss: 0.4257 - val_accuracy: 0.8127\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 18/257 [=>............................] - ETA: 42:25 - loss: 0.4277 - accuracy: 0.8264"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\major_project\\a.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/major_project/a.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, Y_val), callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating the accuracy on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\swapn\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# model.save('model_vgg16_1_fake_detection.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 23s 2s/step\n",
            "Overall Accuracy on Validation Set: 83.000000%\n",
            "Classification Report on Validation Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.87      0.75      0.81       189\n",
            "        real       0.80      0.90      0.85       211\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.83      0.83      0.83       400\n",
            "\n",
            "Confusion Matrix on Validation Set:\n",
            "[[142  47]\n",
            " [ 21 190]]\n",
            "False Positive Rate (FPR): 24.867725%\n",
            "False Negative Rate (FNR): 9.952607%\n",
            "Cohen's Kappa on Validation Set: 0.656505\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
        "class_names = ['fake', 'real']\n",
        "# After training the model, you can use X_val and Y_val for evaluation\n",
        "Y_pred_val = model.predict(X_val)\n",
        "\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes_val = np.argmax(Y_pred_val, axis=1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true_val = np.argmax(Y_val, axis=1)\n",
        "\n",
        "# Calculate overall accuracy on validation set\n",
        "overall_accuracy_val = accuracy_score(Y_true_val, Y_pred_classes_val)\n",
        "print(f'Overall Accuracy on Validation Set: {overall_accuracy_val * 100:.6f}%')\n",
        "\n",
        "# Calculate other metrics using classification_report\n",
        "report_val = classification_report(Y_true_val, Y_pred_classes_val, target_names=class_names)\n",
        "print('Classification Report on Validation Set:')\n",
        "print(report_val)\n",
        "\n",
        "# Calculate confusion matrix on validation set\n",
        "conf_matrix_val = confusion_matrix(Y_true_val, Y_pred_classes_val)\n",
        "print('Confusion Matrix on Validation Set:')\n",
        "print(conf_matrix_val)\n",
        "\n",
        "true_positive = conf_matrix_val[1, 1]\n",
        "false_positive = conf_matrix_val[0, 1]\n",
        "false_negative = conf_matrix_val[1, 0]\n",
        "true_negative = conf_matrix_val[0, 0]\n",
        "\n",
        "# Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "fpr = false_positive / (false_positive + true_negative)\n",
        "fnr = false_negative / (false_negative + true_positive)\n",
        "\n",
        "print(f'False Positive Rate (FPR): {fpr * 100:.6f}%')\n",
        "print(f'False Negative Rate (FNR): {fnr * 100:.6f}%')\n",
        "\n",
        "# Calculate Cohen's Kappa on validation set\n",
        "cohen_kappa_val = cohen_kappa_score(Y_true_val, Y_pred_classes_val)\n",
        "print(f'Cohen\\'s Kappa on Validation Set: {cohen_kappa_val:.6f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Fake Image .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
